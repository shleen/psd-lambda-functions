{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the 3 CSV files from QS, THE, SH rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# change csv file names as necessary\n",
    "df_qs = pd.read_csv('qs_csv.csv')\n",
    "df_the = pd.read_csv('the_csv.csv')\n",
    "df_shanghai = pd.read_csv('sh_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QS uni name cleaning\n",
    "df_qs['University'] = df_qs['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_qs['University'] = df_qs['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE uni name cleaning\n",
    "df_the['University'] = df_the['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_the['University'] = df_the['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SH uni name cleaning\n",
    "df_shanghai['University'] = df_shanghai['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_shanghai['University'] = df_shanghai['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_shanghai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Define a function to find the best match for a given university name\n",
    "def find_best_match(name, choices):\n",
    "    best_match, score = process.extractOne(name, choices)\n",
    "    if score >= 90:  # Adjust the threshold as needed\n",
    "        return best_match\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "# Get unique university names from both dataframes\n",
    "qs_universities = df_qs['University'].unique()\n",
    "the_universities = df_the['University'].unique()\n",
    "\n",
    "# Create a master list of all unique university names\n",
    "all_universities = list(set(qs_universities))\n",
    "\n",
    "\n",
    "# Standardize university names in df_the\n",
    "df_the['New University'] = df_the.apply(lambda x: find_best_match(x['University'], all_universities), axis=1)\n",
    "\n",
    "# Merge the dataframes based on standardized university names\n",
    "merged_df = pd.merge(df_qs, df_the, on=['University', 'Country'], how='outer')\n",
    "\n",
    "# Select the desired columns\n",
    "merged_df = merged_df[['University', 'Country', 'QS Citations per Paper', 'QS Academic Reputation','QS Employer Reputation',\n",
    "                       'Citations', 'Research', 'Teaching']]\n",
    "\n",
    "# Display the merged dataframe\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique university names from both dataframes\n",
    "shanghai_universities = df_shanghai['University'].unique()\n",
    "merged_universities = merged_df['University'].unique()\n",
    "\n",
    "# Create a master list of all unique university names\n",
    "all_universities = list(set(merged_universities))\n",
    "\n",
    "# Standardize university names in df_the\n",
    "df_shanghai['new University'] = df_shanghai.apply(lambda x: find_best_match(x['University'], all_universities), axis=1)\n",
    "\n",
    "# Merge the dataframes based on standardized university names\n",
    "new_merged_df = pd.merge(merged_df, df_shanghai, on=['University'], how='outer')\n",
    "\n",
    "# # Select the desired columns\n",
    "new_merged_df = new_merged_df[['University', 'Country', 'QS Citations per Paper', 'QS Academic Reputation','QS Employer Reputation',\n",
    "                       'Citations', 'Research', 'Teaching', 'CNCI', 'TOP']]\n",
    "\n",
    "# Display the merged dataframe\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "new_merged_df.drop_duplicates(inplace=True)\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 \n",
    "country_column = new_merged_df['Country']\n",
    "new_merged_df.fillna(0, inplace=True)\n",
    "new_merged_df[['University', 'Country', 'QS Citations per Paper', \"Citations\", 'CNCI', 'QS Academic Reputation',\n",
    "              'QS Employer Reputation', 'Research', 'Teaching', 'TOP']]\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure scores are of type int\n",
    "new_merged_df['QS Citations per Paper'] = pd.to_numeric(new_merged_df['QS Citations per Paper'], errors='coerce')\n",
    "new_merged_df['Citations'] = pd.to_numeric(new_merged_df['Citations'], errors='coerce')\n",
    "new_merged_df['CNCI'] = pd.to_numeric(new_merged_df['CNCI'], errors='coerce')\n",
    "\n",
    "# Calculate the \"final citation score\" for each row based on the specified logic\n",
    "def calculate_final_score_citations(row):\n",
    "    qs_citations_per_paper = row['QS Citations per Paper']\n",
    "    citations = row['Citations']\n",
    "    cnci = row['CNCI']\n",
    "\n",
    "    if qs_citations_per_paper != 0 and citations != 0 and cnci != 0:\n",
    "        final_score = (((qs_citations_per_paper + citations + cnci) / 3) / 100) * 0.8 + 0.2\n",
    "    elif (qs_citations_per_paper != 0 and citations != 0) or (qs_citations_per_paper != 0 and cnci != 0) or (citations != 0 and cnci != 0):\n",
    "        final_score = (((qs_citations_per_paper + citations + cnci) / 2) / 100) * 0.8 + 0.1\n",
    "    elif qs_citations_per_paper != 0 or citations != 0 or cnci != 0:\n",
    "        final_score = (max(qs_citations_per_paper, citations, cnci) / 100) * 0.8\n",
    "    else:\n",
    "        final_score = 0\n",
    "\n",
    "    return (final_score*100)\n",
    "\n",
    "# Apply the calculation to each row and create a new 'final citation score' column\n",
    "new_merged_df['final citation score'] = new_merged_df.apply(calculate_final_score_citations, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate the \"final reputation score\" for each row based on the specified logic\n",
    "def calculate_final_score_rep(row):\n",
    "    cols = ['TOP', 'QS Academic Reputation', 'QS Employer Reputation', 'Research', 'Teaching']\n",
    "    non_zero_cols = [col for col in cols if row[col] != 0]\n",
    "    non_zero_count = len(non_zero_cols)\n",
    "\n",
    "    # Convert relevant columns to numeric\n",
    "    numeric_cols = row[non_zero_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    if non_zero_count == 5:\n",
    "        final_score = (((numeric_cols.sum() / 5) / 100) * 0.8) + 0.2\n",
    "    elif non_zero_count == 4:\n",
    "        final_score = (((numeric_cols.sum() / 4) / 100) * 0.8) + 0.15\n",
    "    elif non_zero_count == 3:\n",
    "        final_score = (((numeric_cols.sum() / 3) / 100) * 0.8) + 0.1\n",
    "    elif non_zero_count == 2:\n",
    "        final_score = (((numeric_cols.sum() / 2) / 100) * 0.8) + 0.05\n",
    "    elif non_zero_count == 1:\n",
    "        final_score = (numeric_cols[non_zero_cols[0]] / 100) * 0.8\n",
    "    else:\n",
    "        final_score = 0\n",
    "\n",
    "    return (final_score * 100)\n",
    "\n",
    "# Apply the calculation to each row and create a new 'final reputation score' column\n",
    "new_merged_df['final reputation score'] = new_merged_df.apply(calculate_final_score_rep, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall score based on weightage for citations and reputation\n",
    "new_merged_df['overall score'] = 0.2 * new_merged_df['final citation score'] + 0.8 * new_merged_df['final reputation score']\n",
    "new_merged_df.sort_values(by='overall score', ascending=False, inplace=True)\n",
    "new_merged_df.reset_index(drop=True, inplace=True)\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download to csv\n",
    "new_merged_df.to_csv('FINAL_ranking_list.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
