{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPASS C2 (Qualifications criteria) give EP applications 20 points for coming from a \"top tier university\". However, there is a lack of representation for SEA schools (Malaysia, Indonesia, Vietnam, Philipines, China, India) in traditional ranking systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the team is creating a new ranking system using Citations, Academic Reputation and Employer Reputation as criterias to better identify reputable universities in SEA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries (only needed the first time you run; comment out after)\n",
    "# pip install selenium\n",
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "year = dt.datetime.now().year\n",
    "\n",
    "# define wait times for webscrape to run\n",
    "def wait():\n",
    "    time.sleep(3)\n",
    "def long_wait():\n",
    "    time.sleep(5)\n",
    "\n",
    "# click on accept cookies button\n",
    "def close_cookie(driver):\n",
    "    try:\n",
    "        close_btn = driver.find_element(By.XPATH, \"//div[@class='eu-cookie-compliance-buttons']\")\n",
    "        close_btn.click()\n",
    "        wait()\n",
    "    except:\n",
    "        print(\"Cookie button not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up chrome driver (website to scrape)\n",
    "def setup(year):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    prefs = {\"download.default_directory\": \"\"}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(f\"https://www.topuniversities.com/university-rankings/university-subject-rankings/{year}/computer-science-information-systems?&tab=indicators&sort_by=overallscore&order_by=desc\")\n",
    "    driver.maximize_window()\n",
    "    long_wait()\n",
    "    close_cookie(driver)\n",
    "\n",
    "    # Check if the page contains the \"Coming Soon!\" error message\n",
    "    if 'Coming Soon!' in driver.page_source:\n",
    "        year -= 1\n",
    "        driver.quit()\n",
    "        return setup(year)  # Recursively call setup() with the decremented year\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# button to load more data\n",
    "def click_load_more(driver):\n",
    "    while True:\n",
    "        try:\n",
    "            # Check if the \"No data found\" message is present\n",
    "            time.sleep(5)\n",
    "            no_data_msg = driver.find_element(By.XPATH, \"//div[contains(text(), 'No data found on applied filters')]\")\n",
    "            break  # Stop clicking the \"Load More\" button if the message is found\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            load_more_btn = driver.find_element(By.XPATH, \"//button[contains(@class, 'loadmorebutton') and contains(text(), 'Load More')]\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", load_more_btn)\n",
    "            driver.execute_script(\"arguments[0].click();\", load_more_btn)\n",
    "            wait()\n",
    "        except NoSuchElementException:\n",
    "            # If the \"Load More\" button is not found, break the loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# button to scroll right on data table to get other criterias\n",
    "def table_click_right(driver):\n",
    "  right_arrow = driver.find_elements(By.XPATH, \"//span[@direction='right']\")\n",
    "  right_arrow[0].click()\n",
    "  wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run driver\n",
    "driver = setup(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load more data\n",
    "click_load_more(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets full table of data through the class name\n",
    "data = driver.find_elements(By.XPATH, \"//*[@class='td-wrap-in']\")\n",
    "# for i in range(len(data)):\n",
    "#     print(i, data[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get school names\n",
    "schools_data = list()\n",
    "for i in range(1, len(data), 8):\n",
    "    schools_data.append(data[i].text)\n",
    "# schools_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll right of table to get more criterias\n",
    "table_click_right(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Academic Reputation scores\n",
    "AR_data = list()\n",
    "for i in range(3, len(data), 8):\n",
    "    AR_data.append(data[i].text)\n",
    "# AR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Employer Reputation scores\n",
    "ER_data = list()\n",
    "for i in range(4, len(data), 8):\n",
    "    ER_data.append(data[i].text)\n",
    "# ER_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Citations scores\n",
    "citations_data = list()\n",
    "for i in range(5, len(data), 8):\n",
    "    citations_data.append(data[i].text)\n",
    "# citations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get locations of schools\n",
    "def extract_location(driver):\n",
    "    locations = driver.find_elements(By.XPATH, \"//div[contains(@class, 'location')]\")\n",
    "    location_data = []\n",
    "    for location in locations:\n",
    "        location_text = location.text.strip()\n",
    "        if location_text:\n",
    "            location_data.append(location_text)\n",
    "    return location_data\n",
    "\n",
    "location_data = extract_location(driver)\n",
    "# print(location_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating countries and cities from location data\n",
    "city_list = []\n",
    "country_list = []\n",
    "\n",
    "for location in location_data:\n",
    "    # Check if the location contains a comma\n",
    "    if ',' in location:\n",
    "        # Split the string at the comma\n",
    "        split_location = location.split(',')\n",
    "\n",
    "        # Remove leading and trailing whitespaces from the city and country\n",
    "        city = split_location[0].strip()\n",
    "        country = split_location[1].strip()\n",
    "\n",
    "        city_list.append(city)\n",
    "        country_list.append(country)\n",
    "    else:\n",
    "        city_list.append(location)\n",
    "        country_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_qs = pd.DataFrame()\n",
    "\n",
    "# Populating data into dataframe\n",
    "df_qs[\"University\"] = schools_data\n",
    "df_qs[\"QS Citations per Paper\"] = citations_data\n",
    "df_qs[\"QS Academic Reputation\"] = AR_data\n",
    "df_qs[\"QS Employer Reputation\"] = ER_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the \"Country\" column\n",
    "df_qs['Country'] = df_qs['Country'].replace(\"China\", \"China (Mainland)\")\n",
    "df_qs['Country'] = country_list\n",
    "df_qs = df_qs[['University','Country', \"QS Citations per Paper\",\n",
    "         \"QS Academic Reputation\", \"QS Employer Reputation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset\n",
    "df_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import datetime as dt\n",
    "year = dt.datetime.now().year\n",
    "\n",
    "# Define wait times for webscrape to run\n",
    "def wait():\n",
    "    time.sleep(3)\n",
    "def long_wait():\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up chrome driver (website to scrape)\n",
    "def setup(year):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--ignore-certificate-errors')\n",
    "    chrome_options.add_argument('--ignore-ssl-errors')\n",
    "\n",
    "    prefs = {\"download.default_directory\": \"\"}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(f\"https://www.timeshighereducation.com/world-university-rankings/{year}/subject-ranking/computer-science#!/length/-1/sort_by/rank/sort_order/asc/cols/stats\")\n",
    "    driver.maximize_window()\n",
    "    long_wait()\n",
    "\n",
    "    # Check if the page displays the error message\n",
    "    if \"Sorry, we couldn't find that page. Try searching instead.\" in driver.page_source:\n",
    "        year -= 1\n",
    "        driver.quit()\n",
    "        return setup(year)  # Recursively call setup() with the decremented year\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button to click on \"scores\" data\n",
    "def table_click_scores(driver):\n",
    "  scores = driver.find_elements(By.XPATH, \"//label[contains(@for, 'scores')]\")\n",
    "  scores[0].click()\n",
    "  wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch chromedriver\n",
    "driver = setup(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show scores menu\n",
    "table_click_scores(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get School names\n",
    "df_name = driver.find_elements(By.XPATH, \"//*[contains(@class, 'ranking-institution-title')]\")\n",
    "name_data = list()\n",
    "for i in range(len(df_name)):\n",
    "    name_data.append(df_name[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Country\n",
    "df_country = driver.find_elements(By.XPATH, \"//div/span\")\n",
    "country_data = list()\n",
    "for i in range(len(df_country)):\n",
    "    country_data.append(df_country[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Citation scores\n",
    "df_citations = driver.find_elements(By.XPATH, \"//td[contains(@class, 'scores citations-score')]\")\n",
    "citations_data = list()\n",
    "for i in range(len(df_citations)):\n",
    "    citations_data.append(df_citations[i].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Research scores\n",
    "df_research = driver.find_elements(By.XPATH, \"//td[contains(@class, 'scores research-score')]\")\n",
    "research_data = list()\n",
    "for i in range(len(df_research)):\n",
    "    research_data.append(df_research[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Teaching scores\n",
    "df_teaching = driver.find_elements(By.XPATH, \"//td[contains(@class, 'scores teaching-score')]\")\n",
    "teaching_data = list()\n",
    "for i in range(len(df_teaching)):\n",
    "    teaching_data.append(df_teaching[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_the = pd.DataFrame()\n",
    "\n",
    "# Populating data into dataframe\n",
    "df_the[\"University\"] = name_data\n",
    "df_the[\"Country\"] = country_data\n",
    "df_the[\"Citations\"] = citations_data\n",
    "df_the[\"Research\"] = research_data\n",
    "df_the[\"Teaching\"] = teaching_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset\n",
    "df_the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "year = dt.datetime.now().year\n",
    "\n",
    "# define wait times for webscrape to run\n",
    "def wait():\n",
    "    time.sleep(3)\n",
    "def long_wait():\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up chrome driver (website to scrape)\n",
    "def setup(year):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    prefs = {\"download.default_directory\": \"\"}\n",
    "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(f\"https://www.shanghairanking.com/rankings/gras/{year}/RS0210\")\n",
    "    driver.maximize_window()\n",
    "    long_wait()\n",
    "    \n",
    "    # Check if the page is inaccessible\n",
    "    if \"Sorry, the page is inaccessible\" in driver.page_source:\n",
    "        year -= 1\n",
    "        driver.quit()\n",
    "        return setup(year)  # Recursively call setup() with the decremented year\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button to click \"CNCI\" scores\n",
    "def click_cnci(driver):\n",
    "    try:\n",
    "        button = driver.find_element(By.XPATH, '//*[@id=\"content-box\"]//th[5]//img')\n",
    "        button.click()\n",
    "        long_wait()\n",
    "        cnci = driver.find_element(By.XPATH, '//li[text()=\"CNCI\"]')\n",
    "        cnci.click()\n",
    "        wait()\n",
    "    except Exception as e:\n",
    "        print(\"cnci button not found\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button to click \"TOP\" scores\n",
    "def click_top(driver):\n",
    "    try:\n",
    "        button = driver.find_element(By.XPATH, '//*[@id=\"content-box\"]//th[5]//img')\n",
    "        button.click()\n",
    "        long_wait()\n",
    "        top = driver.find_element(By.XPATH, '//li[text()=\"TOP\"]')\n",
    "        top.click()\n",
    "        wait()\n",
    "    except Exception as e:\n",
    "        print(\"top button not found\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run driver\n",
    "driver = setup(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Button to go back to first page\n",
    "back_to_first_page = driver.find_element(By.XPATH, '//li[@title=\"1\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get School names\n",
    "def extract_names(driver):\n",
    "    university_names = []\n",
    "    while True:\n",
    "        rows = driver.find_elements(By.XPATH, '//tr[@data-v-ae1ab4a8=\"\"]')\n",
    "        for row in rows:\n",
    "            university_name_elements = row.find_elements(By.XPATH, './/span[@class=\"univ-name\"]')\n",
    "            for element in university_name_elements:\n",
    "                university_name = element.text.strip()\n",
    "                university_names.append(university_name)  # Append the university name to the list\n",
    "\n",
    "        try:\n",
    "            next_page_button = driver.find_element(By.XPATH, '//li[@title=\"下一页\"]')\n",
    "            if next_page_button.get_attribute('aria-disabled') == 'true':\n",
    "                print(\"Next Page button is not clickable. Stopping loop.\")\n",
    "                break\n",
    "            next_page_button.click()\n",
    "            wait()\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"No more pages available.\", e)\n",
    "            break\n",
    "    \n",
    "    return university_names\n",
    "\n",
    "back_to_first_page.click()\n",
    "university = extract_names(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CNCI scores\n",
    "def extract_cnci(driver):\n",
    "    cnci_numbers = []\n",
    "    while True:\n",
    "        rows = driver.find_elements(By.XPATH, '//tr[@data-v-ae1ab4a8=\"\"]')\n",
    "        for row in rows:\n",
    "            td_elements = row.find_elements(By.XPATH, './/td')\n",
    "            if len(td_elements) >= 5:\n",
    "                cnci_element = td_elements[4]  # Select the fifth td element (which is the cnci number)\n",
    "                cnci_number = cnci_element.text.strip()\n",
    "                cnci_numbers.append(cnci_number)  # Append the CNCI number to the list\n",
    "\n",
    "        try:\n",
    "            next_page_button = driver.find_element(By.XPATH, '//li[@title=\"下一页\"]')\n",
    "            if next_page_button.get_attribute('aria-disabled') == 'true':\n",
    "                print(\"Next Page button is not clickable. Stopping loop.\")\n",
    "                break\n",
    "            next_page_button.click()\n",
    "            wait()\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"No more pages available.\", e)\n",
    "            break\n",
    "\n",
    "    return cnci_numbers\n",
    "\n",
    "\n",
    "back_to_first_page.click()\n",
    "time.sleep(12)\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "time.sleep(3)\n",
    "click_cnci(driver)\n",
    "time.sleep(10)\n",
    "cnci = extract_cnci(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TOP scores\n",
    "def extract_top(driver):\n",
    "    top_numbers = []\n",
    "    while True:\n",
    "        rows = driver.find_elements(By.XPATH, '//tr[@data-v-ae1ab4a8=\"\"]')\n",
    "        for row in rows:\n",
    "            td_elements = row.find_elements(By.XPATH, './/td')\n",
    "            if len(td_elements) >= 5:\n",
    "                top_element = td_elements[4]  # Select the fifth td element\n",
    "                top_number = top_element.text.strip()\n",
    "                top_numbers.append(top_number)  # Append the top number to the list\n",
    "\n",
    "        try:\n",
    "            next_page_button = driver.find_element(By.XPATH, '//li[@title=\"下一页\"]')\n",
    "            if next_page_button.get_attribute('aria-disabled') == 'true':\n",
    "                print(\"Next Page button is not clickable. Stopping loop.\")\n",
    "                break\n",
    "            next_page_button.click()\n",
    "            wait()\n",
    "        except NoSuchElementException as e:\n",
    "            print(\"No more pages available.\", e)\n",
    "            break\n",
    "\n",
    "    return top_numbers\n",
    "\n",
    "back_to_first_page.click()\n",
    "time.sleep(12)\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "time.sleep(3)\n",
    "click_top(driver)\n",
    "time.sleep(10)\n",
    "top = extract_top(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_shanghai = pd.DataFrame()\n",
    "\n",
    "# Populating data into dataframe\n",
    "df_shanghai[\"University\"] = university\n",
    "df_shanghai[\"CNCI\"] = cnci\n",
    "df_shanghai[\"TOP\"] = top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset\n",
    "df_shanghai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the 3 CSV files from QS, THE, SH rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QS uni name cleaning\n",
    "df_qs['University'] = df_qs['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_qs['University'] = df_qs['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE uni name cleaning\n",
    "df_the['University'] = df_the['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_the['University'] = df_the['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SH uni name cleaning\n",
    "df_shanghai['University'] = df_shanghai['University'].str.lower().replace('\\s\\(.*\\)', '', regex=True)\n",
    "df_shanghai['University'] = df_shanghai['University'].str.lower().replace('[^a-z0-9 ]', '', regex=True)\n",
    "df_shanghai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Define a function to find the best match for a given university name\n",
    "def find_best_match(name, choices):\n",
    "    best_match, score = process.extractOne(name, choices)\n",
    "    if score >= 90:  # Adjust the threshold as needed\n",
    "        return best_match\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "# Get unique university names from both dataframes\n",
    "qs_universities = df_qs['University'].unique()\n",
    "the_universities = df_the['University'].unique()\n",
    "\n",
    "# Create a master list of all unique university names\n",
    "all_universities = list(set(qs_universities))\n",
    "\n",
    "\n",
    "# Standardize university names in df_the\n",
    "df_the['New University'] = df_the.apply(lambda x: find_best_match(x['University'], all_universities), axis=1)\n",
    "\n",
    "# Merge the dataframes based on standardized university names\n",
    "merged_df = pd.merge(df_qs, df_the, on=['University', 'Country'], how='outer')\n",
    "\n",
    "# Select the desired columns\n",
    "merged_df = merged_df[['University', 'Country', 'QS Citations per Paper', 'QS Academic Reputation','QS Employer Reputation',\n",
    "                       'Citations', 'Research', 'Teaching']]\n",
    "\n",
    "# Display the merged dataframe\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique university names from both dataframes\n",
    "shanghai_universities = df_shanghai['University'].unique()\n",
    "merged_universities = merged_df['University'].unique()\n",
    "\n",
    "# Create a master list of all unique university names\n",
    "all_universities = list(set(merged_universities))\n",
    "\n",
    "# Standardize university names in df_the\n",
    "df_shanghai['new University'] = df_shanghai.apply(lambda x: find_best_match(x['University'], all_universities), axis=1)\n",
    "\n",
    "# Merge the dataframes based on standardized university names\n",
    "new_merged_df = pd.merge(merged_df, df_shanghai, on=['University'], how='outer')\n",
    "\n",
    "# # Select the desired columns\n",
    "new_merged_df = new_merged_df[['University', 'Country', 'QS Citations per Paper', 'QS Academic Reputation','QS Employer Reputation',\n",
    "                       'Citations', 'Research', 'Teaching', 'CNCI', 'TOP']]\n",
    "\n",
    "# Display the merged dataframe\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "new_merged_df.drop_duplicates(inplace=True)\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 \n",
    "country_column = new_merged_df['Country']\n",
    "new_merged_df.fillna(0, inplace=True)\n",
    "new_merged_df[['University', 'Country', 'QS Citations per Paper', \"Citations\", 'CNCI', 'QS Academic Reputation',\n",
    "              'QS Employer Reputation', 'Research', 'Teaching', 'TOP']]\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure scores are of type int\n",
    "new_merged_df['QS Citations per Paper'] = pd.to_numeric(new_merged_df['QS Citations per Paper'], errors='coerce')\n",
    "new_merged_df['Citations'] = pd.to_numeric(new_merged_df['Citations'], errors='coerce')\n",
    "new_merged_df['CNCI'] = pd.to_numeric(new_merged_df['CNCI'], errors='coerce')\n",
    "\n",
    "# Calculate the \"final citation score\" for each row based on the specified logic\n",
    "def calculate_final_score_citations(row):\n",
    "    qs_citations_per_paper = row['QS Citations per Paper']\n",
    "    citations = row['Citations']\n",
    "    cnci = row['CNCI']\n",
    "\n",
    "    if qs_citations_per_paper != 0 and citations != 0 and cnci != 0:\n",
    "        final_score = (((qs_citations_per_paper + citations + cnci) / 3) / 100) * 0.8 + 0.2\n",
    "    elif (qs_citations_per_paper != 0 and citations != 0) or (qs_citations_per_paper != 0 and cnci != 0) or (citations != 0 and cnci != 0):\n",
    "        final_score = (((qs_citations_per_paper + citations + cnci) / 2) / 100) * 0.8 + 0.1\n",
    "    elif qs_citations_per_paper != 0 or citations != 0 or cnci != 0:\n",
    "        final_score = (max(qs_citations_per_paper, citations, cnci) / 100) * 0.8\n",
    "    else:\n",
    "        final_score = 0\n",
    "\n",
    "    return (final_score*100)\n",
    "\n",
    "# Apply the calculation to each row and create a new 'final citation score' column\n",
    "new_merged_df['final citation score'] = new_merged_df.apply(calculate_final_score_citations, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate the \"final reputation score\" for each row based on the specified logic\n",
    "def calculate_final_score_rep(row):\n",
    "    cols = ['TOP', 'QS Academic Reputation', 'QS Employer Reputation', 'Research', 'Teaching']\n",
    "    non_zero_cols = [col for col in cols if row[col] != 0]\n",
    "    non_zero_count = len(non_zero_cols)\n",
    "\n",
    "    # Convert relevant columns to numeric\n",
    "    numeric_cols = row[non_zero_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    if non_zero_count == 5:\n",
    "        final_score = (((numeric_cols.sum() / 5) / 100) * 0.8) + 0.2\n",
    "    elif non_zero_count == 4:\n",
    "        final_score = (((numeric_cols.sum() / 4) / 100) * 0.8) + 0.15\n",
    "    elif non_zero_count == 3:\n",
    "        final_score = (((numeric_cols.sum() / 3) / 100) * 0.8) + 0.1\n",
    "    elif non_zero_count == 2:\n",
    "        final_score = (((numeric_cols.sum() / 2) / 100) * 0.8) + 0.05\n",
    "    elif non_zero_count == 1:\n",
    "        final_score = (numeric_cols[non_zero_cols[0]] / 100) * 0.8\n",
    "    else:\n",
    "        final_score = 0\n",
    "\n",
    "    return (final_score * 100)\n",
    "\n",
    "# Apply the calculation to each row and create a new 'final reputation score' column\n",
    "new_merged_df['final reputation score'] = new_merged_df.apply(calculate_final_score_rep, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall score based on weightage for citations and reputation\n",
    "new_merged_df['overall score'] = 0.2 * new_merged_df['final citation score'] + 0.8 * new_merged_df['final reputation score']\n",
    "new_merged_df.sort_values(by='overall score', ascending=False, inplace=True)\n",
    "new_merged_df.reset_index(drop=True, inplace=True)\n",
    "new_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download to csv\n",
    "new_merged_df.to_csv('FINAL_ranking_list.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
